import { SizeLimit } from './SizeLimit';
import { Term } from './Term';
import { getInput } from './utils';
import { context, getOctokit } from '@actions/github';
import { setFailed } from '@actions/core';
import { markdownTable as table } from 'markdown-table';

const SIZE_LIMIT_HEADING = `## size-limit report ðŸ“¦ `;

async function run() {
  const { payload, repo } = context;

  // èŽ·å–æœ¬æ¬¡ pr ç›¸å…³ä¿¡æ¯
  const pr = payload.pull_request;

  if (!pr) {
    throw new Error('No PR found. Only pull_request workflows are supported.');
  }

  const token = getInput('github_token');
  // const skipStep = getInput('skip_step');
  const buildScript = getInput('build_script');
  const cleanScript = getInput('clean_script');
  const packageManager = getInput('package_manager');
  const directory = getInput('directory') || process.cwd();

  // åˆå§‹åŒ– github å®¢æˆ·ç«¯â‰ =
  const octokit = getOctokit(token);
  const term = new Term();

  // å½“å‰æäº¤åˆ†æ”¯ä¸‹èŽ·å– size-limit æŠ¥å‘Š
  const { status, output } = await term.execSizeLimit(
    null,
    buildScript,
    cleanScript,
    directory,
    packageManager
  );
  // pr target åˆ†æ”¯ä¸‹èŽ·å– size-limit æŠ¥å‘Š
  const { output: baseOutput } = await term.execSizeLimit(
    pr.base.ref, // éœ€è¦åˆ‡æ¢åˆ°çš„ç›®æ ‡åˆ†æ”¯
    buildScript,
    cleanScript,
    directory,
    packageManager
  );

  const limit = new SizeLimit();

  let base;
  let current;
  try {
    // å°† base å’Œ current çš„æŠ¥å‘Šæ ¼å¼åŒ–æˆä¸ºä¾¿äºŽæ“ä½œçš„ json object
    base = limit.parseResults(baseOutput);
    current = limit.parseResults(output);
  } catch (error) {
    console.log(
      'Error parsing size-limit output. The output should be a json.'
    );
    throw error;
  }

  const body = [
    SIZE_LIMIT_HEADING,
    // ç”Ÿæˆ size-limit markdown table å½¢å¼çš„å‰åŽå¯¹æ¯”æŠ¥å‘Š
    table(limit.formatResults(base, current))
  ].join('\r\n');

  // èŽ·å–å½“å‰ PR ä¸‹çš„æ‰€æœ‰è¯„è®º
  const commentLists = await octokit.paginate(
    'GET /repos/:owner/:repo/issues/:issue_number/comments',
    {
      ...repo,
      issue_number: pr.number
    }
  );
  // åˆ¤æ–­å½“å‰ PR ä¸‹æ‰€æœ‰è¯„è®ºå†…å®¹æ˜¯å¦åŒ…å« size-limit æŠ¥å‘Š
  const sizeLimitComment = commentLists.find((comment) =>
    (comment as any).body.startsWith(SIZE_LIMIT_HEADING)
  );

  const comment = !sizeLimitComment ? null : sizeLimitComment;

  if (!sizeLimitComment) {
    try {
      // ä¸º PR å…³è”çš„ issues åˆ›å»ºä¸€æ¡è¯„è®ºï¼Œå†…å®¹ä¸º size-limit æŠ¥å‘Š
      // è¯¥æ¡è¯„è®ºä¼šåŒæ­¥åœ¨ PullRequest ä¸‹
      await octokit.rest.issues.createComment({
        ...repo,
        // eslint-disable-next-line camelcase
        issue_number: pr.number,
        body
      });
    } catch (error) {
      console.log(
        "Error creating comment. This can happen for PR's originating from a fork without write permissions."
      );
    }
  } else {
    try {
      // ä¸º PR å…³è”çš„ issues æ›´æ–° size-limit è¯„è®ºå†…å®¹
      await octokit.rest.issues.updateComment({
        ...repo,
        // eslint-disable-next-line camelcase
        comment_id: (comment as any).id,
        body
      });
    } catch (error) {
      console.log(
        "Error updating comment. This can happen for PR's originating from a fork without write permissions."
      );
    }
  }

  if (status > 0) {
    setFailed('Size limit has been exceeded.');
  }
}

run();
